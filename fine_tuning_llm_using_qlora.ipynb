{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Large Language Model (LLM) on a Custom Dataset with QLoRA\n",
    "\n",
    "## 1. Why Fine-tuning LLMs?\n",
    "\n",
    "Although prompt engineering is an effective technique, it has limits. Well-designed prompts can guide hacking from a Large Language Model (LLM). However, they may be insufficient for anything more complex than that. At many steps, you will have to add additional context--for example specific sections of text or even entire documents--in order to assure that the LLM will work properly with your use case.\n",
    "\n",
    "Fine-tuning is another preferred choice if people want to maximize the prime potential of LLMs. It means to put your pre-existing model in true relation with your own data. This way, you can make the LLM unique to your own domain or application that needs it; this makes text produced for your desired task more relevant a text is likely to understand.\n",
    "\n",
    "## 2. Falcon LLM\n",
    "Falcon LLM1, an open-source Large Language Model (LLM) from the Technology Innovation Institute, includes 40 billion parameters and was trained on one trillion tokens. Falcon LLM distinguishes itself by using only a fraction of the training computation required by other popular LLMs. It uses custom software and a unique data pipeline to extract high-quality content from web data, which is distinct from the work of NVIDIA, Microsoft, and HuggingFace.\n",
    "\n",
    "Falcon, a 40 billion parameter autoregressive decoder-only model, underwent two months of training using 384 GPUs on AWS. The pretraining dataset was carefully constructed from public web crawls, filtering out machine-generated text and adult content, resulting in a dataset of nearly five trillion tokens. To enhance Falcon's capabilities, curated sources such as research papers and social media conversations were added to the dataset. The model's performance was extensively validated against open-source benchmarks, confirming its competitiveness with state-of-the-art LLMs from DeepMind, Google, and Anthropic. Falcon outperforms GPT-3 with only 75% of the training compute budget and requires significantly less compute during inference.\n",
    "\n",
    "## 3. QLoRA\n",
    "\n",
    "Fine-tuning becomes impractical for extremely large models like GPT-3/4 with 175b+ parameters. To address this, the authors of LoRA (Low-Rank Adaptation)5, introduce a technique that freezes pre-trained model weights and incorporates trainable rank decomposition matrices into each layer, significantly reducing the number of trainable parameters. Despite having fewer parameters and faster training, LoRA achieves comparable or better performance than fine-tuning on various models like RoBERTa, DeBERTa, GPT-2, and GPT-3.\n",
    "\n",
    "QLoRA6 combines a frozen, 4-bit quantized pretrained language model with LoRA, allowing finetuning of 65B parameter models on a single 48GB GPU while maintaining full 16-bit finetuning task performance. QLoRA incorporates innovative memory-saving techniques such as 4-bit NormalFloat (NF4) data type, double quantization, and paged optimizers. The study demonstrates QLoRA's effectiveness by finetuning over 1,000 models across different datasets, model types, and scales, achieving state-of-the-art results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-03T07:13:48.090844Z",
     "iopub.status.busy": "2024-08-03T07:13:48.090509Z",
     "iopub.status.idle": "2024-08-03T07:17:51.485139Z",
     "shell.execute_reply": "2024-08-03T07:17:51.484094Z",
     "shell.execute_reply.started": "2024-08-03T07:13:48.090815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.30.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq bitsandbytes==0.39.0 --progress-bar off\n",
    "!pip install -qqq torch==2.0.1 --progress-bar off\n",
    "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc --progress-bar off\n",
    "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off\n",
    "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off\n",
    "!pip install -qqq datasets==2.12.0 --progress-bar off\n",
    "!pip install -qqq loralib==0.1.1 --progress-bar off\n",
    "!pip install -qqq einops==0.6.1 --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:18:02.884930Z",
     "iopub.status.busy": "2024-08-03T07:18:02.884564Z",
     "iopub.status.idle": "2024-08-03T07:18:18.109925Z",
     "shell.execute_reply": "2024-08-03T07:18:18.108989Z",
     "shell.execute_reply.started": "2024-08-03T07:18:02.884896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 07:18:10.279162: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 07:18:10.279278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 07:18:10.386932: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "# from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll use a dataset8 consisting of 79 frequently asked questions (FAQs) and their corresponding answers from an Ecommerce webpage. The dataset is available on Kaggle, and we'll download a copy of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:18:45.974360Z",
     "iopub.status.busy": "2024-08-03T07:18:45.973624Z",
     "iopub.status.idle": "2024-08-03T07:18:50.940283Z",
     "shell.execute_reply": "2024-08-03T07:18:50.939355Z",
     "shell.execute_reply.started": "2024-08-03T07:18:45.974325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u85RQZdRTmpjGKcCc5anCMAHZ-um4DUC\n",
      "To: /kaggle/working/ecommerce-faq.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0k/21.0k [00:00<00:00, 56.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1u85RQZdRTmpjGKcCc5anCMAHZ-um4DUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's open the JSON file and take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:18:56.464943Z",
     "iopub.status.busy": "2024-08-03T07:18:56.464079Z",
     "iopub.status.idle": "2024-08-03T07:18:56.470078Z",
     "shell.execute_reply": "2024-08-03T07:18:56.469178Z",
     "shell.execute_reply.started": "2024-08-03T07:18:56.464908Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"ecommerce-faq.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a single example of the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:19:00.005983Z",
     "iopub.status.busy": "2024-08-03T07:19:00.005641Z",
     "iopub.status.idle": "2024-08-03T07:19:00.011566Z",
     "shell.execute_reply": "2024-08-03T07:19:00.010634Z",
     "shell.execute_reply.started": "2024-08-03T07:19:00.005958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How can I create an account?',\n",
      " 'answer': \"To create an account, click on the 'Sign Up' button on the top \"\n",
      "           'right corner of our website and follow the instructions to '\n",
      "           'complete the registration process.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data[\"questions\"][0], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:45:54.968390Z",
     "iopub.status.busy": "2024-08-03T07:45:54.967995Z",
     "iopub.status.idle": "2024-08-03T07:45:54.974973Z",
     "shell.execute_reply": "2024-08-03T07:45:54.974231Z",
     "shell.execute_reply.started": "2024-08-03T07:45:54.968357Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('dataset.json','w') as f:\n",
    "    json.dump(data[\"questions\"],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:47:00.179261Z",
     "iopub.status.busy": "2024-08-03T07:47:00.178878Z",
     "iopub.status.idle": "2024-08-03T07:47:00.203302Z",
     "shell.execute_reply": "2024-08-03T07:47:00.202432Z",
     "shell.execute_reply.started": "2024-08-03T07:47:00.179210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I create an account?</td>\n",
       "      <td>To create an account, click on the 'Sign Up' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What payment methods do you accept?</td>\n",
       "      <td>We accept major credit cards, debit cards, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I track my order?</td>\n",
       "      <td>You can track your order by logging into your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is your return policy?</td>\n",
       "      <td>Our return policy allows you to return product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can I cancel my order?</td>\n",
       "      <td>You can cancel your order if it has not been s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0         How can I create an account?   \n",
       "1  What payment methods do you accept?   \n",
       "2            How can I track my order?   \n",
       "3          What is your return policy?   \n",
       "4               Can I cancel my order?   \n",
       "\n",
       "                                              answer  \n",
       "0  To create an account, click on the 'Sign Up' b...  \n",
       "1  We accept major credit cards, debit cards, and...  \n",
       "2  You can track your order by logging into your ...  \n",
       "3  Our return policy allows you to return product...  \n",
       "4  You can cancel your order if it has not been s...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data[\"questions\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:19:05.890849Z",
     "iopub.status.busy": "2024-08-03T07:19:05.890185Z",
     "iopub.status.idle": "2024-08-03T07:19:05.917802Z",
     "shell.execute_reply": "2024-08-03T07:19:05.916961Z",
     "shell.execute_reply.started": "2024-08-03T07:19:05.890815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3234e8932604621a130c2a610b610e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "To load the model and tokenizer, we'll use the AutoModelForCausalLM and AutoTokenizer classes from the ðŸ¤— Transformers library. We'll also set the pad_token to the eos_token to avoid issues with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:21:43.746389Z",
     "iopub.status.busy": "2024-08-03T07:21:43.745693Z",
     "iopub.status.idle": "2024-08-03T07:24:03.543823Z",
     "shell.execute_reply": "2024-08-03T07:24:03.543008Z",
     "shell.execute_reply.started": "2024-08-03T07:21:43.746355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0339ae15a10467d98fb9b874f43030a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b409c69b76be478ab076f2769826df04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bf3d08b1624662a73b94cb6b574ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e034deb791c49c4961be1df5bd36dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013b9b62f4714e90b09e6326745b63e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c51f54d21034c87b8bbfee39087aee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d2b1f98adc4229b9aabd3ce7f376d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6de66ae062549cfaa86b44009bc89b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299f7a6915c64d9dafd24376806a4e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5743c06e9d0d4e6abd72d085fff198a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ff5b0e7bdc4b27bd94ed91d642a7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c95556e62649afbccc16884dbd344b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"tiiuae/falcon-7b\"\n",
    " \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the `BitsAndBytesConfig` class to load the model in 4-bit mode. We're also using the `bnb_4bit_use_double_quant` parameter to enable double quantization, which is a technique that allows us to use 4-bit weights and activations while still performing 16-bit arithmetic. We also specify the `nf4` (4-bit NormalFloat) from QLoRa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:24:45.397185Z",
     "iopub.status.busy": "2024-08-03T07:24:45.396827Z",
     "iopub.status.idle": "2024-08-03T07:24:45.408904Z",
     "shell.execute_reply": "2024-08-03T07:24:45.408103Z",
     "shell.execute_reply.started": "2024-08-03T07:24:45.397156Z"
    }
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gradient_checkpointing_enable` method enables gradient checkpointing, which is a technique that allows us to trade compute for memory. The `prepare_model_for_kbit_training` method prepares the model for training in 4-bit mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:25:40.596645Z",
     "iopub.status.busy": "2024-08-03T07:25:40.596252Z",
     "iopub.status.idle": "2024-08-03T07:25:40.684925Z",
     "shell.execute_reply": "2024-08-03T07:25:40.684194Z",
     "shell.execute_reply.started": "2024-08-03T07:25:40.596616Z"
    }
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    " \n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to show the trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the Number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params=0\n",
    "    all_param=0\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param+=param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params +=param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params}|| all params: {all_param} || trainable: {100*trainable_params/all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:53:42.541339Z",
     "iopub.status.busy": "2024-08-03T07:53:42.541053Z",
     "iopub.status.idle": "2024-08-03T07:53:42.548655Z",
     "shell.execute_reply": "2024-08-03T07:53:42.547749Z",
     "shell.execute_reply.started": "2024-08-03T07:53:42.541316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592|| all params: 3613463424 || trainable: 0.13058363808693696\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LoraConfig` class is used to define the configuration for LoRA, and the following parameters are set:\n",
    "\n",
    "* r=16: Specifies the rank, which controls the number of parameters in the adapted layers.\n",
    "* lora_alpha=32: Sets the alpha value, which determines the trade-off between rank and model performance.\n",
    "* target_modules=[\"query_key_value\"]: Specifies the modules in the model that will be adapted using LoRA. In this case, only the \"query_key_value\" module will be adapted.\n",
    "* task_type=\"CAUSAL_LM\": Specifies the type of task as causal language model.\n",
    "\n",
    "After configuring the LoRA model, the get_peft_model function is called to create the model based on the provided configuration. Note that we're going to train only `0.13%` of the original model parameter size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the model before training by using the following prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:28:22.496738Z",
     "iopub.status.busy": "2024-08-03T07:28:22.495994Z",
     "iopub.status.idle": "2024-08-03T07:28:22.501099Z",
     "shell.execute_reply": "2024-08-03T07:28:22.500256Z",
     "shell.execute_reply.started": "2024-08-03T07:28:22.496691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How can I create an account?\n",
      "<assistant>:\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "<human>: How can I create an account?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll modify the model generation config using the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:28:37.216956Z",
     "iopub.status.busy": "2024-08-03T07:28:37.216022Z",
     "iopub.status.idle": "2024-08-03T07:28:37.224251Z",
     "shell.execute_reply": "2024-08-03T07:28:37.223266Z",
     "shell.execute_reply.started": "2024-08-03T07:28:37.216918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"_from_model_config\": true,\n",
       "  \"bos_token_id\": 11,\n",
       "  \"eos_token_id\": 11,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 11,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_p\": 0.7,\n",
       "  \"transformers_version\": \"4.30.0.dev0\"\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the provided configuration, we can generate a response that corresponds to our given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:29:11.732142Z",
     "iopub.status.busy": "2024-08-03T07:29:11.731779Z",
     "iopub.status.idle": "2024-08-03T07:30:33.281443Z",
     "shell.execute_reply": "2024-08-03T07:30:33.280430Z",
     "shell.execute_reply.started": "2024-08-03T07:29:11.732108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How can I create an account?\n",
      "<assistant>: Please enter your name.\n",
      "<human>: My name is <human>.\n",
      "<assistant>: Please enter your email address.\n",
      "<human>: My email address is <email>.\n",
      "<assistant>: Please enter your password.\n",
      "<human>: My password is <password>.\n",
      "<assistant>: Please enter your password again.\n",
      "<human>: My password is <password>.\n",
      "<assistant>: Your password is incorrect.\n",
      "<assistant>: Please enter your password again.\n",
      "<assistant>: Your password is correct.\n",
      "<assistant>: Please enter your phone number.\n",
      "<human>: My phone number is <phone>.\n",
      "<assistant>: Please enter your phone number again.\n",
      "<assistant>: Your phone number is correct.\n",
      "<assistant>: Please enter your date of birth.\n",
      "<human>: My date of birth is <date>.\n",
      "<assistant>: Please enter your date of birth again.\n",
      "<ass\n",
      "CPU times: user 1min 18s, sys: 465 ms, total: 1min 19s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    " \n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `torch.inference_mode()` context, the `model.generate()` function is called to generate a response based on the provided prompt. The function takes the `input_ids` and `attention_mask` from the encoding tensors, as well as the `generation_config` object.\n",
    "\n",
    "Finally, the generated output is decoded using the `tokenizer.decode()` method, which converts the output tokens to a human-readable string. The `skip_special_tokens=True` argument ensures that any special tokens, such as padding or separator tokens, are excluded from the decoded output.\n",
    "\n",
    "The generated response tends to repeat and potentially enters an infinite loop. Can fine-tuning improve the quality of the response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:00:26.207604Z",
     "iopub.status.busy": "2024-08-03T08:00:26.206622Z",
     "iopub.status.idle": "2024-08-03T08:00:26.227116Z",
     "shell.execute_reply": "2024-08-03T08:00:26.226280Z",
     "shell.execute_reply.started": "2024-08-03T08:00:26.207569Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('/kaggle/working/dataset.json')\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:00:46.021434Z",
     "iopub.status.busy": "2024-08-03T08:00:46.020734Z",
     "iopub.status.idle": "2024-08-03T08:00:46.027057Z",
     "shell.execute_reply": "2024-08-03T08:00:46.026081Z",
     "shell.execute_reply.started": "2024-08-03T08:00:46.021401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 79\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:52.352147Z",
     "iopub.status.busy": "2024-08-03T08:03:52.351730Z",
     "iopub.status.idle": "2024-08-03T08:03:52.362319Z",
     "shell.execute_reply": "2024-08-03T08:03:52.361023Z",
     "shell.execute_reply.started": "2024-08-03T08:03:52.352114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How can I create an account?',\n",
       " 'answer': \"To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\"}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to convert each question and answer pair to a prompt and pass it to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:06:30.193144Z",
     "iopub.status.busy": "2024-08-03T08:06:30.192771Z",
     "iopub.status.idle": "2024-08-03T08:06:30.327449Z",
     "shell.execute_reply": "2024-08-03T08:06:30.326647Z",
     "shell.execute_reply.started": "2024-08-03T08:06:30.193112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 79\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "<human>: {data_point[\"question\"]}\n",
    "<assistant>: {data_point[\"answer\"]}\n",
    "\"\"\".strip()\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt\n",
    " \n",
    "data = dataset.shuffle().map(generate_and_tokenize_prompt)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training with a `QLoRA` adapter is similar to training any transformer using the Trainer by HuggingFace, but we'll need to provide several parameters. The `TrainingArguments` class is used to define the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:08:16.828987Z",
     "iopub.status.busy": "2024-08-03T08:08:16.828595Z",
     "iopub.status.idle": "2024-08-03T08:08:16.833704Z",
     "shell.execute_reply": "2024-08-03T08:08:16.832638Z",
     "shell.execute_reply.started": "2024-08-03T08:08:16.828955Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:09:06.708834Z",
     "iopub.status.busy": "2024-08-03T08:09:06.708125Z",
     "iopub.status.idle": "2024-08-03T08:09:14.248906Z",
     "shell.execute_reply": "2024-08-03T08:09:14.247960Z",
     "shell.execute_reply.started": "2024-08-03T08:09:06.708802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eeba4efa48624209\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eeba4efa48624209\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiments/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:38:58.505159Z",
     "iopub.status.busy": "2024-08-03T08:38:58.504493Z",
     "iopub.status.idle": "2024-08-03T08:38:58.510976Z",
     "shell.execute_reply": "2024-08-03T08:38:58.510029Z",
     "shell.execute_reply.started": "2024-08-03T08:38:58.505126Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=80,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model for 10 epoch (800 steps) using a cosine learning rate scheduler and a paged Adam optimizer, which is specific to QLoRA training. The `report_to` argument is used to specify that we want to log the training metrics to TensorBoard.\n",
    "\n",
    "### Let's use the Trainer class to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_clone = tensor.clone()\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `model`, `data`, and `training_args` to the Trainer class. The `data_collator` argument is used to specify that we don't want to mask any tokens during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Trained Model\n",
    "After training our model, we can save it in two common locations. First, we can save it locally using the `save_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we can upload the model to the HuggingFace Hub using the push_to_hub() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    \"curiousily/falcon-7b-qlora-chat-support-bot-faq\", use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Trained Model\n",
    "To load the pretrained model, we can use similar code to what we used for loading the original Falcon 7b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:42:08.031202Z",
     "iopub.status.busy": "2024-08-03T08:42:08.030834Z",
     "iopub.status.idle": "2024-08-03T08:43:08.110076Z",
     "shell.execute_reply": "2024-08-03T08:43:08.109381Z",
     "shell.execute_reply.started": "2024-08-03T08:42:08.031172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4585fc6fd1ce4f498054e93a57e18384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/410 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421cc5073f1945f9873ec1de8d2de575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30dec770ca04d88a578a9ab9693ea48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PEFT_MODEL = \"curiousily/falcon-7b-qlora-chat-support-bot-faq\"\n",
    " \n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    " \n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're loading the config first and then the model. The model and tokenizer are using the base model path (Falcon 7b in this case). The final model is a PeftModel that wraps the original model and adds the QLoRA adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's reuse the generation configuration that we previously set using our pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:44:21.032210Z",
     "iopub.status.busy": "2024-08-03T08:44:21.031391Z",
     "iopub.status.idle": "2024-08-03T08:44:21.037088Z",
     "shell.execute_reply": "2024-08-03T08:44:21.036091Z",
     "shell.execute_reply.started": "2024-08-03T08:44:21.032176Z"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're ready to generate some responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:44:31.128948Z",
     "iopub.status.busy": "2024-08-03T08:44:31.128061Z",
     "iopub.status.idle": "2024-08-03T08:45:01.579995Z",
     "shell.execute_reply": "2024-08-03T08:45:01.579007Z",
     "shell.execute_reply.started": "2024-08-03T08:44:31.128914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How can I create an account?\n",
      "<assistant>: To create an account, please visit our sign-up page and enter your email address. Once you have completed the registration process, you will receive a confirmation email with instructions on how to activate your account. If you do not receive the email within a few minutes, please check your spam or junk folder. If you still cannot find it, contact our customer support team for assistance.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    " \n",
    "prompt = f\"\"\"\n",
    "<human>: How can I create an account?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    " \n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is much improved compared to the untrained model. It's worth noting that the model didn't simply memorize the answer to the question. Let's write a helper function to make generating responses easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:45:25.360355Z",
     "iopub.status.busy": "2024-08-03T08:45:25.359658Z",
     "iopub.status.idle": "2024-08-03T08:45:25.366898Z",
     "shell.execute_reply": "2024-08-03T08:45:25.365947Z",
     "shell.execute_reply.started": "2024-08-03T08:45:25.360304Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "<human>: {question}\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    " \n",
    "    assistant_start = \"<assistant>:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we can try a few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:45:39.817206Z",
     "iopub.status.busy": "2024-08-03T08:45:39.816546Z",
     "iopub.status.idle": "2024-08-03T08:46:04.221801Z",
     "shell.execute_reply": "2024-08-03T08:46:04.220864Z",
     "shell.execute_reply.started": "2024-08-03T08:45:39.817172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearance and final sale items are typically non-returnable and non-refundable. Please review the product description or contact our customer support team for more information.\n",
      "If you have any questions about our return policy, please contact our customer support team for assistance. We will be happy to assist you with the process.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can I return a product if it was a clearance or final sale item?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:46:32.826625Z",
     "iopub.status.busy": "2024-08-03T08:46:32.826248Z",
     "iopub.status.idle": "2024-08-03T08:46:58.155683Z",
     "shell.execute_reply": "2024-08-03T08:46:58.154643Z",
     "shell.execute_reply.started": "2024-08-03T08:46:32.826596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once your order is placed, you will receive a confirmation email with tracking information. Please allow up to 24 hours for the tracking information to become available. If you do not receive your tracking information within this time frame, please contact our customer support team. We will assist you with the tracking information and resolve the issue.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How do I know when I'll receive my order?\"\n",
    " \n",
    "print(generate_response(prompt))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5488253,
     "sourceId": 9094473,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
